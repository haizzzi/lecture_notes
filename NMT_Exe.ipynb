{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMF/PfBrAftxxZxRrhPFtWs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haizzzi/simple-nmt-simplied/blob/main/NMT_Exe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "제일먼저 구글드라이버에 nmt-fold에 code1과 data1이 설치되도록 자신의 구글드라이버에 제공되는 zip 파일을 압축 해제해서 upload 하세요. 그리고 좌측 mount 버튼을 누르세요. 그러면 구글 드라이브 마운트 위치 \"/content/drive/MyDrive/\"에 마운트가 되어서 /content/drive/MyDrive/nmt_fold/data1/ 형태로 접근이 됩니다."
      ],
      "metadata": {
        "id": "hXKDzevwElOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# 이미 마운트돼 있으면 다시 연결하고 싶을 때 force_remount=True로\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install torch\n",
        "!pip install torch_optimizer\n",
        "!pip install pytorch-ignite"
      ],
      "metadata": {
        "id": "CoHvjUX6mkcZ",
        "outputId": "d9e51aae-9504-4685-93cf-ed72625f0aa3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m131.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m99.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m109.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127\n",
            "Collecting torch_optimizer\n",
            "  Downloading torch_optimizer-0.3.0-py3-none-any.whl.metadata (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from torch_optimizer) (2.6.0+cu124)\n",
            "Collecting pytorch-ranger>=0.1.1 (from torch_optimizer)\n",
            "  Downloading pytorch_ranger-0.1.1-py3-none-any.whl.metadata (509 bytes)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.5.0->torch_optimizer) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.5.0->torch_optimizer) (3.0.2)\n",
            "Downloading torch_optimizer-0.3.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_ranger-0.1.1-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pytorch-ranger, torch_optimizer\n",
            "Successfully installed pytorch-ranger-0.1.1 torch_optimizer-0.3.0\n",
            "Collecting pytorch-ignite\n",
            "  Downloading pytorch_ignite-0.5.2-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: torch<3,>=1.3 in /usr/local/lib/python3.11/dist-packages (from pytorch-ignite) (2.6.0+cu124)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pytorch-ignite) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=1.3->pytorch-ignite) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=1.3->pytorch-ignite) (3.0.2)\n",
            "Downloading pytorch_ignite-0.5.2-py3-none-any.whl (343 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m343.2/343.2 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytorch-ignite\n",
            "Successfully installed pytorch-ignite-0.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)\n"
      ],
      "metadata": {
        "id": "MVaPUSLB8YNu",
        "outputId": "77e70177-2010-4873-8bf8-4bf563e48cbf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0+cu124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "seq2seq 학습 시키는 코드\n",
        "10epochs 학습시키면 ppl이 5정도로 내려감.\n",
        "원래 batch_size가 160이었으나, A100 40GB에 맞춰 960으로 바꿔서 학습 시킴"
      ],
      "metadata": {
        "id": "Q9Btzoq1HiXo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAUvswrLkv9m",
        "outputId": "2799b596-8f48-4767-8597-1b2f921e7a5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{   'batch_size': 160,\n",
            "    'dropout': 0.2,\n",
            "    'gpu_id': 0,\n",
            "    'hidden_size': 768,\n",
            "    'init_epoch': 1,\n",
            "    'iteration_per_update': 2,\n",
            "    'lang': 'enko',\n",
            "    'lr': 0.001,\n",
            "    'lr_decay_start': 10,\n",
            "    'lr_gamma': 0.5,\n",
            "    'lr_step': 0,\n",
            "    'max_grad_norm': 100000000.0,\n",
            "    'max_length': 64,\n",
            "    'model_fn': '/content/drive/MyDrive/nmt_fold/data1/models1.seq2seq.pth',\n",
            "    'n_epochs': 30,\n",
            "    'n_layers': 4,\n",
            "    'n_splits': 8,\n",
            "    'off_autocast': False,\n",
            "    'rl_lr': 0.01,\n",
            "    'rl_n_epochs': 10,\n",
            "    'rl_n_gram': 6,\n",
            "    'rl_n_samples': 1,\n",
            "    'rl_reward': 'gleu',\n",
            "    'train': '/content/drive/MyDrive/nmt_fold/data1/combined.shuf.train.tok.bpe',\n",
            "    'use_adam': True,\n",
            "    'use_radam': False,\n",
            "    'use_transformer': False,\n",
            "    'valid': '/content/drive/MyDrive/nmt_fold/data1/combined.shuf.valid.tok.bpe',\n",
            "    'verbose': 2,\n",
            "    'word_vec_size': 512}\n",
            "Seq2Seq(\n",
            "  (emb_src): Embedding(20264, 512)\n",
            "  (emb_dec): Embedding(22291, 512)\n",
            "  (encoder): Encoder(\n",
            "    (rnn): LSTM(512, 384, num_layers=4, batch_first=True, dropout=0.2, bidirectional=True)\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (rnn): LSTM(1280, 768, num_layers=4, batch_first=True, dropout=0.2)\n",
            "  )\n",
            "  (attn): Attention(\n",
            "    (linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "    (softmax): Softmax(dim=-1)\n",
            "  )\n",
            "  (concat): Linear(in_features=1536, out_features=768, bias=True)\n",
            "  (tanh): Tanh()\n",
            "  (generator): Generator(\n",
            "    (output): Linear(in_features=768, out_features=22291, bias=True)\n",
            "    (softmax): LogSoftmax(dim=-1)\n",
            "  )\n",
            ")\n",
            "NLLLoss()\n",
            "Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    weight_decay: 0\n",
            ")\n",
            "/content/drive/MyDrive/nmt_fold/simple-nmt/simple_nmt/trainer.py:34: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = GradScaler()\n",
            "/content/drive/MyDrive/nmt_fold/simple-nmt/simple_nmt/trainer.py:58: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(not engine.config.off_autocast):\n",
            "Epoch [1/30]:   1%|     | 43/6779 [00:26<1:07:38,  1.66it/s, loss=8.23, ppl=8.08e+3, |param|=4.67e+3, |g_param|=7.31e+5]Engine run is terminating due to exception: \n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/nmt_fold/simple-nmt/train.py\", line 361, in <module>\n",
            "    main(config)\n",
            "  File \"/content/drive/MyDrive/nmt_fold/simple-nmt/train.py\", line 331, in main\n",
            "    mle_trainer.train(\n",
            "  File \"/content/drive/MyDrive/nmt_fold/simple-nmt/simple_nmt/trainer.py\", line 311, in train\n",
            "    train_engine.run(train_loader, max_epochs=n_epochs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ignite/engine/engine.py\", line 905, in run\n",
            "    return self._internal_run()\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ignite/engine/engine.py\", line 948, in _internal_run\n",
            "    return next(self._internal_run_generator)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ignite/engine/engine.py\", line 1023, in _internal_run_as_gen\n",
            "    self._handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ignite/engine/engine.py\", line 660, in _handle_exception\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ignite/engine/engine.py\", line 972, in _internal_run_as_gen\n",
            "    epoch_time_taken += yield from self._run_once_on_dataset_as_gen()\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ignite/engine/engine.py\", line 1110, in _run_once_on_dataset_as_gen\n",
            "    self.state.output = self._process_function(self, self.state.batch)\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/nmt_fold/simple-nmt/simple_nmt/trainer.py\", line 62, in train\n",
            "    y_hat = engine.model(x, mini_batch.tgt[0][:, :-1])\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/nmt_fold/simple-nmt/simple_nmt/models/seq2seq.py\", line 265, in forward\n",
            "    h_src, h_0_tgt = self.encoder((emb_src, x_length))\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/nmt_fold/simple-nmt/simple_nmt/models/seq2seq.py\", line 65, in forward\n",
            "    x = pack(x, lengths.tolist(), batch_first=True)\n",
            "                ^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "!python /content/drive/MyDrive/nmt_fold/simple-nmt/train.py --train /content/drive/MyDrive/nmt_fold/data1/combined.shuf.train.tok.bpe --valid /content/drive/MyDrive/nmt_fold/data1/combined.shuf.valid.tok.bpe --lang enko --gpu_id 0 --batch_size 160 --n_epochs 30 --max_length 64 --dropout .2 --word_vec_size 512 --hidden_size 768 --n_layers 4 --max_grad_norm 1e+8 --iteration_per_update 2 --lr 1e-3 --lr_step 0 --use_adam --model_fn /content/drive/MyDrive/nmt_fold/data1/models1.seq2seq.pth --rl_n_epochs 0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "trasformer학습 시키는 코드\n",
        "10epochs 학습 시켜도 ppl이 17정도로 만족스럽지 않아서 10epochs 더 학습 시킴. batch_size가 원래 128이었으나, A100 40GB에 맞춰서 768으로 바꿔서 학습 시킴"
      ],
      "metadata": {
        "id": "VeqqzmE5He3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/nmt_fold/simple-nmt/train.py --train /content/drive/MyDrive/nmt_fold/data1/combined.shuf.train.tok.bpe --valid /content/drive/MyDrive/nmt_fold/data1/combined.shuf.valid.tok.bpe --lang enko --gpu_id 0 --batch_size 768 --n_epochs 10 --max_length 64 --dropout .2 --hidden_size 768 --n_layers 4 --max_grad_norm 1e+8 --iteration_per_update 32 --lr 1e-3 --lr_step 0 --use_adam --rl_n_epochs 0 --use_transformer --model_fn /content/drive/MyDrive/nmt_fold/data1/models1.transformer.pth"
      ],
      "metadata": {
        "id": "_PfN0bpFHeJp",
        "outputId": "d259eebe-2881-4b68-bfb0-05e5eb7c64e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{   'batch_size': 128,\n",
            "    'dropout': 0.2,\n",
            "    'gpu_id': 0,\n",
            "    'hidden_size': 768,\n",
            "    'init_epoch': 1,\n",
            "    'iteration_per_update': 32,\n",
            "    'lang': 'enko',\n",
            "    'lr': 0.001,\n",
            "    'lr_decay_start': 10,\n",
            "    'lr_gamma': 0.5,\n",
            "    'lr_step': 0,\n",
            "    'max_grad_norm': 100000000.0,\n",
            "    'max_length': 64,\n",
            "    'model_fn': '/content/drive/MyDrive/nmt_fold/data1/models1.transformer.pth',\n",
            "    'n_epochs': 30,\n",
            "    'n_layers': 4,\n",
            "    'n_splits': 8,\n",
            "    'off_autocast': False,\n",
            "    'rl_lr': 0.01,\n",
            "    'rl_n_epochs': 0,\n",
            "    'rl_n_gram': 6,\n",
            "    'rl_n_samples': 1,\n",
            "    'rl_reward': 'gleu',\n",
            "    'train': '/content/drive/MyDrive/nmt_fold/data1/combined.shuf.train.tok.bpe',\n",
            "    'use_adam': True,\n",
            "    'use_radam': False,\n",
            "    'use_transformer': True,\n",
            "    'valid': '/content/drive/MyDrive/nmt_fold/data1/combined.shuf.valid.tok.bpe',\n",
            "    'verbose': 2,\n",
            "    'word_vec_size': 512}\n",
            "Transformer(\n",
            "  (emb_enc): Embedding(20264, 768)\n",
            "  (emb_dec): Embedding(22291, 768)\n",
            "  (emb_dropout): Dropout(p=0.2, inplace=False)\n",
            "  (encoder): MySequential(\n",
            "    (0): EncoderBlock(\n",
            "      (attn): MultiHead(\n",
            "        (Q_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (K_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (V_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (attn): Attention(\n",
            "          (softmax): Softmax(dim=-1)\n",
            "        )\n",
            "      )\n",
            "      (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (attn_dropout): Dropout(p=0.2, inplace=False)\n",
            "      (fc): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (1): ReLU()\n",
            "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "      )\n",
            "      (fc_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (fc_dropout): Dropout(p=0.2, inplace=False)\n",
            "    )\n",
            "    (1): EncoderBlock(\n",
            "      (attn): MultiHead(\n",
            "        (Q_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (K_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (V_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (attn): Attention(\n",
            "          (softmax): Softmax(dim=-1)\n",
            "        )\n",
            "      )\n",
            "      (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (attn_dropout): Dropout(p=0.2, inplace=False)\n",
            "      (fc): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (1): ReLU()\n",
            "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "      )\n",
            "      (fc_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (fc_dropout): Dropout(p=0.2, inplace=False)\n",
            "    )\n",
            "    (2): EncoderBlock(\n",
            "      (attn): MultiHead(\n",
            "        (Q_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (K_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (V_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (attn): Attention(\n",
            "          (softmax): Softmax(dim=-1)\n",
            "        )\n",
            "      )\n",
            "      (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (attn_dropout): Dropout(p=0.2, inplace=False)\n",
            "      (fc): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (1): ReLU()\n",
            "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "      )\n",
            "      (fc_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (fc_dropout): Dropout(p=0.2, inplace=False)\n",
            "    )\n",
            "    (3): EncoderBlock(\n",
            "      (attn): MultiHead(\n",
            "        (Q_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (K_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (V_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (attn): Attention(\n",
            "          (softmax): Softmax(dim=-1)\n",
            "        )\n",
            "      )\n",
            "      (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (attn_dropout): Dropout(p=0.2, inplace=False)\n",
            "      (fc): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (1): ReLU()\n",
            "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "      )\n",
            "      (fc_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (fc_dropout): Dropout(p=0.2, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (decoder): MySequential(\n",
            "    (0): DecoderBlock(\n",
            "      (masked_attn): MultiHead(\n",
            "        (Q_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (K_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (V_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (attn): Attention(\n",
            "          (softmax): Softmax(dim=-1)\n",
            "        )\n",
            "      )\n",
            "      (masked_attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (masked_attn_dropout): Dropout(p=0.2, inplace=False)\n",
            "      (attn): MultiHead(\n",
            "        (Q_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (K_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (V_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (attn): Attention(\n",
            "          (softmax): Softmax(dim=-1)\n",
            "        )\n",
            "      )\n",
            "      (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (attn_dropout): Dropout(p=0.2, inplace=False)\n",
            "      (fc): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (1): ReLU()\n",
            "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "      )\n",
            "      (fc_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (fc_dropout): Dropout(p=0.2, inplace=False)\n",
            "    )\n",
            "    (1): DecoderBlock(\n",
            "      (masked_attn): MultiHead(\n",
            "        (Q_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (K_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (V_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (attn): Attention(\n",
            "          (softmax): Softmax(dim=-1)\n",
            "        )\n",
            "      )\n",
            "      (masked_attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (masked_attn_dropout): Dropout(p=0.2, inplace=False)\n",
            "      (attn): MultiHead(\n",
            "        (Q_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (K_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (V_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (attn): Attention(\n",
            "          (softmax): Softmax(dim=-1)\n",
            "        )\n",
            "      )\n",
            "      (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (attn_dropout): Dropout(p=0.2, inplace=False)\n",
            "      (fc): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (1): ReLU()\n",
            "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "      )\n",
            "      (fc_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (fc_dropout): Dropout(p=0.2, inplace=False)\n",
            "    )\n",
            "    (2): DecoderBlock(\n",
            "      (masked_attn): MultiHead(\n",
            "        (Q_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (K_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (V_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (attn): Attention(\n",
            "          (softmax): Softmax(dim=-1)\n",
            "        )\n",
            "      )\n",
            "      (masked_attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (masked_attn_dropout): Dropout(p=0.2, inplace=False)\n",
            "      (attn): MultiHead(\n",
            "        (Q_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (K_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (V_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (attn): Attention(\n",
            "          (softmax): Softmax(dim=-1)\n",
            "        )\n",
            "      )\n",
            "      (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (attn_dropout): Dropout(p=0.2, inplace=False)\n",
            "      (fc): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (1): ReLU()\n",
            "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "      )\n",
            "      (fc_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (fc_dropout): Dropout(p=0.2, inplace=False)\n",
            "    )\n",
            "    (3): DecoderBlock(\n",
            "      (masked_attn): MultiHead(\n",
            "        (Q_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (K_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (V_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (attn): Attention(\n",
            "          (softmax): Softmax(dim=-1)\n",
            "        )\n",
            "      )\n",
            "      (masked_attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (masked_attn_dropout): Dropout(p=0.2, inplace=False)\n",
            "      (attn): MultiHead(\n",
            "        (Q_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (K_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (V_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (attn): Attention(\n",
            "          (softmax): Softmax(dim=-1)\n",
            "        )\n",
            "      )\n",
            "      (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (attn_dropout): Dropout(p=0.2, inplace=False)\n",
            "      (fc): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (1): ReLU()\n",
            "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "      )\n",
            "      (fc_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (fc_dropout): Dropout(p=0.2, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (generator): Sequential(\n",
            "    (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "    (1): Linear(in_features=768, out_features=22291, bias=True)\n",
            "    (2): LogSoftmax(dim=-1)\n",
            "  )\n",
            ")\n",
            "NLLLoss()\n",
            "Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.98)\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    weight_decay: 0\n",
            ")\n",
            "/content/drive/MyDrive/nmt_fold/simple-nmt/simple_nmt/trainer.py:34: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = GradScaler()\n",
            "/content/drive/MyDrive/nmt_fold/simple-nmt/simple_nmt/trainer.py:58: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(not engine.config.off_autocast):\n",
            "Epoch [1/30]:   4%|▍         | 362/8474 [02:05<48:35,  2.78it/s, loss=6.56, ppl=737, |param|=5.72e+3, |g_param|=8.61e+5]Engine run is terminating due to exception: \n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/nmt_fold/simple-nmt/train.py\", line 361, in <module>\n",
            "    main(config)\n",
            "  File \"/content/drive/MyDrive/nmt_fold/simple-nmt/train.py\", line 331, in main\n",
            "    mle_trainer.train(\n",
            "  File \"/content/drive/MyDrive/nmt_fold/simple-nmt/simple_nmt/trainer.py\", line 311, in train\n",
            "    train_engine.run(train_loader, max_epochs=n_epochs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ignite/engine/engine.py\", line 905, in run\n",
            "    return self._internal_run()\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ignite/engine/engine.py\", line 948, in _internal_run\n",
            "    return next(self._internal_run_generator)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ignite/engine/engine.py\", line 1023, in _internal_run_as_gen\n",
            "    self._handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ignite/engine/engine.py\", line 660, in _handle_exception\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ignite/engine/engine.py\", line 972, in _internal_run_as_gen\n",
            "    epoch_time_taken += yield from self._run_once_on_dataset_as_gen()\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ignite/engine/engine.py\", line 1110, in _run_once_on_dataset_as_gen\n",
            "    self.state.output = self._process_function(self, self.state.batch)\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/nmt_fold/simple-nmt/simple_nmt/trainer.py\", line 77, in train\n",
            "    p_norm = float(get_parameter_norm(engine.model.parameters()))\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/nmt_fold/simple-nmt/simple_nmt/utils.py\", line 27, in get_parameter_norm\n",
            "    total_norm += (p.data**norm_type).sum()\n",
            "    ^^^^^^^^^^\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "연속해서 학습하는 코드. 10epochs 학습 시킨 모델을 시작으로 20epochs 까지 학습한다. 10epochs에 A100으로 1시반 30분 정도라서 절반씩 나눠서 학습 했다. 20epochs 학습 시키면 ppl이 5.01"
      ],
      "metadata": {
        "id": "adovHaazfCnV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/nmt_fold/simple-nmt/continue_train.py --load_fn /content/drive/MyDrive/nmt_fold/data1/models1.transformer10.pth --model_fn /content/drive/MyDrive/nmt_fold/data1/models1.transformer.pth --init_epoch 11 --n_epochs 20 --use_transformer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POEQwe--Vpz3",
        "outputId": "97e6b8b6-0d76-4be2-fd13-e6ca1b4a8c2c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING!!! Argument \"--load_fn\" is not found in saved model.\tUse current value: /content/drive/MyDrive/nmt_fold/data1/models1.transformer10.pth\n",
            "WARNING!!! You changed value for argument \"--n_epochs\".\tUse current value: 20\n",
            "WARNING!!! You changed value for argument \"--init_epoch\".\tUse current value: 11\n",
            "{   'batch_size': 768,\n",
            "    'dropout': 0.2,\n",
            "    'gpu_id': 0,\n",
            "    'hidden_size': 768,\n",
            "    'init_epoch': 11,\n",
            "    'iteration_per_update': 32,\n",
            "    'lang': 'enko',\n",
            "    'load_fn': '/content/drive/MyDrive/nmt_fold/data1/models1.transformer10.pth',\n",
            "    'lr': 0.001,\n",
            "    'lr_decay_start': 10,\n",
            "    'lr_gamma': 0.5,\n",
            "    'lr_step': 0,\n",
            "    'max_grad_norm': 100000000.0,\n",
            "    'max_length': 64,\n",
            "    'model_fn': '/content/drive/MyDrive/nmt_fold/data1/models1.transformer.pth',\n",
            "    'n_epochs': 20,\n",
            "    'n_layers': 4,\n",
            "    'n_splits': 8,\n",
            "    'off_autocast': False,\n",
            "    'rl_lr': 0.01,\n",
            "    'rl_n_epochs': 0,\n",
            "    'rl_n_gram': 6,\n",
            "    'rl_n_samples': 1,\n",
            "    'rl_reward': 'gleu',\n",
            "    'train': '/content/drive/MyDrive/nmt_fold/data1/combined.shuf.train.tok.bpe',\n",
            "    'use_adam': True,\n",
            "    'use_radam': False,\n",
            "    'use_transformer': True,\n",
            "    'valid': '/content/drive/MyDrive/nmt_fold/data1/combined.shuf.valid.tok.bpe',\n",
            "    'verbose': 2,\n",
            "    'word_vec_size': 512}\n",
            "Transformer(\n",
            "  (emb_enc): Embedding(20264, 768)\n",
            "  (emb_dec): Embedding(22291, 768)\n",
            "  (emb_dropout): Dropout(p=0.2, inplace=False)\n",
            "  (encoder): MySequential(\n",
            "    (0): EncoderBlock(\n",
            "      (attn): MultiHead(\n",
            "        (Q_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (K_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (V_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (attn): Attention(\n",
            "          (softmax): Softmax(dim=-1)\n",
            "        )\n",
            "      )\n",
            "      (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (attn_dropout): Dropout(p=0.2, inplace=False)\n",
            "      (fc): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (1): ReLU()\n",
            "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "      )\n",
            "      (fc_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (fc_dropout): Dropout(p=0.2, inplace=False)\n",
            "    )\n",
            "    (1): EncoderBlock(\n",
            "      (attn): MultiHead(\n",
            "        (Q_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (K_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (V_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (attn): Attention(\n",
            "          (softmax): Softmax(dim=-1)\n",
            "        )\n",
            "      )\n",
            "      (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (attn_dropout): Dropout(p=0.2, inplace=False)\n",
            "      (fc): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (1): ReLU()\n",
            "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "      )\n",
            "      (fc_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (fc_dropout): Dropout(p=0.2, inplace=False)\n",
            "    )\n",
            "    (2): EncoderBlock(\n",
            "      (attn): MultiHead(\n",
            "        (Q_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (K_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (V_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (attn): Attention(\n",
            "          (softmax): Softmax(dim=-1)\n",
            "        )\n",
            "      )\n",
            "      (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (attn_dropout): Dropout(p=0.2, inplace=False)\n",
            "      (fc): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (1): ReLU()\n",
            "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "      )\n",
            "      (fc_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (fc_dropout): Dropout(p=0.2, inplace=False)\n",
            "    )\n",
            "    (3): EncoderBlock(\n",
            "      (attn): MultiHead(\n",
            "        (Q_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (K_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (V_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (attn): Attention(\n",
            "          (softmax): Softmax(dim=-1)\n",
            "        )\n",
            "      )\n",
            "      (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (attn_dropout): Dropout(p=0.2, inplace=False)\n",
            "      (fc): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (1): ReLU()\n",
            "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "      )\n",
            "      (fc_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (fc_dropout): Dropout(p=0.2, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (decoder): MySequential(\n",
            "    (0): DecoderBlock(\n",
            "      (masked_attn): MultiHead(\n",
            "        (Q_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (K_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (V_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (attn): Attention(\n",
            "          (softmax): Softmax(dim=-1)\n",
            "        )\n",
            "      )\n",
            "      (masked_attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (masked_attn_dropout): Dropout(p=0.2, inplace=False)\n",
            "      (attn): MultiHead(\n",
            "        (Q_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (K_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (V_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (attn): Attention(\n",
            "          (softmax): Softmax(dim=-1)\n",
            "        )\n",
            "      )\n",
            "      (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (attn_dropout): Dropout(p=0.2, inplace=False)\n",
            "      (fc): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (1): ReLU()\n",
            "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "      )\n",
            "      (fc_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (fc_dropout): Dropout(p=0.2, inplace=False)\n",
            "    )\n",
            "    (1): DecoderBlock(\n",
            "      (masked_attn): MultiHead(\n",
            "        (Q_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (K_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (V_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (attn): Attention(\n",
            "          (softmax): Softmax(dim=-1)\n",
            "        )\n",
            "      )\n",
            "      (masked_attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (masked_attn_dropout): Dropout(p=0.2, inplace=False)\n",
            "      (attn): MultiHead(\n",
            "        (Q_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (K_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (V_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (attn): Attention(\n",
            "          (softmax): Softmax(dim=-1)\n",
            "        )\n",
            "      )\n",
            "      (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (attn_dropout): Dropout(p=0.2, inplace=False)\n",
            "      (fc): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (1): ReLU()\n",
            "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "      )\n",
            "      (fc_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (fc_dropout): Dropout(p=0.2, inplace=False)\n",
            "    )\n",
            "    (2): DecoderBlock(\n",
            "      (masked_attn): MultiHead(\n",
            "        (Q_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (K_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (V_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (attn): Attention(\n",
            "          (softmax): Softmax(dim=-1)\n",
            "        )\n",
            "      )\n",
            "      (masked_attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (masked_attn_dropout): Dropout(p=0.2, inplace=False)\n",
            "      (attn): MultiHead(\n",
            "        (Q_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (K_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (V_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (attn): Attention(\n",
            "          (softmax): Softmax(dim=-1)\n",
            "        )\n",
            "      )\n",
            "      (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (attn_dropout): Dropout(p=0.2, inplace=False)\n",
            "      (fc): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (1): ReLU()\n",
            "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "      )\n",
            "      (fc_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (fc_dropout): Dropout(p=0.2, inplace=False)\n",
            "    )\n",
            "    (3): DecoderBlock(\n",
            "      (masked_attn): MultiHead(\n",
            "        (Q_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (K_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (V_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (attn): Attention(\n",
            "          (softmax): Softmax(dim=-1)\n",
            "        )\n",
            "      )\n",
            "      (masked_attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (masked_attn_dropout): Dropout(p=0.2, inplace=False)\n",
            "      (attn): MultiHead(\n",
            "        (Q_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (K_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (V_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (attn): Attention(\n",
            "          (softmax): Softmax(dim=-1)\n",
            "        )\n",
            "      )\n",
            "      (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (attn_dropout): Dropout(p=0.2, inplace=False)\n",
            "      (fc): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (1): ReLU()\n",
            "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "      )\n",
            "      (fc_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (fc_dropout): Dropout(p=0.2, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (generator): Sequential(\n",
            "    (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "    (1): Linear(in_features=768, out_features=22291, bias=True)\n",
            "    (2): LogSoftmax(dim=-1)\n",
            "  )\n",
            ")\n",
            "NLLLoss()\n",
            "Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.98)\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: True\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    weight_decay: 0\n",
            ")\n",
            "/content/drive/MyDrive/nmt_fold/simple-nmt/simple_nmt/trainer.py:34: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = GradScaler()\n",
            "/content/drive/MyDrive/nmt_fold/simple-nmt/simple_nmt/trainer.py:58: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(not engine.config.off_autocast):\n",
            "Epoch 11 - |param|=5.75e+03 |g_param|=3.93e+05 loss=2.8199e+00 ppl=16.78\n",
            "/content/drive/MyDrive/nmt_fold/simple-nmt/simple_nmt/trainer.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(not engine.config.off_autocast):\n",
            "Validation - loss=2.7669e+00 ppl=15.91 best_loss=inf best_ppl=inf\n",
            "Epoch 12 - |param|=5.75e+03 |g_param|=2.84e+05 loss=2.5636e+00 ppl=12.98\n",
            "Validation - loss=2.4908e+00 ppl=12.07 best_loss=2.7669e+00 best_ppl=15.91\n",
            "Epoch 13 - |param|=5.75e+03 |g_param|=7.56e+05 loss=2.4257e+00 ppl=11.31\n",
            "Validation - loss=2.3670e+00 ppl=10.67 best_loss=2.4908e+00 best_ppl=12.07\n",
            "Epoch 14 - |param|=5.75e+03 |g_param|=4.31e+05 loss=2.1806e+00 ppl=8.85\n",
            "Validation - loss=2.1454e+00 ppl=8.55 best_loss=2.3670e+00 best_ppl=10.67\n",
            "Epoch 15 - |param|=5.76e+03 |g_param|=1.70e+05 loss=2.0150e+00 ppl=7.50\n",
            "Validation - loss=1.9660e+00 ppl=7.14 best_loss=2.1454e+00 best_ppl=8.55\n",
            "Epoch 16 - |param|=5.76e+03 |g_param|=4.07e+05 loss=1.9764e+00 ppl=7.22\n",
            "Validation - loss=1.9221e+00 ppl=6.84 best_loss=1.9660e+00 best_ppl=7.14\n",
            "Epoch 17 - |param|=5.76e+03 |g_param|=2.38e+05 loss=1.8349e+00 ppl=6.26\n",
            "Validation - loss=1.8008e+00 ppl=6.05 best_loss=1.9221e+00 best_ppl=6.84\n",
            "Epoch 18 - |param|=5.76e+03 |g_param|=1.86e+05 loss=1.7148e+00 ppl=5.56\n",
            "Validation - loss=1.6970e+00 ppl=5.46 best_loss=1.8008e+00 best_ppl=6.05\n",
            "Epoch 19 - |param|=5.77e+03 |g_param|=2.69e+05 loss=1.6581e+00 ppl=5.25\n",
            "Validation - loss=1.6544e+00 ppl=5.23 best_loss=1.6970e+00 best_ppl=5.46\n",
            "Epoch 20 - |param|=5.77e+03 |g_param|=2.31e+05 loss=1.5885e+00 ppl=4.90\n",
            "Validation - loss=1.6120e+00 ppl=5.01 best_loss=1.6544e+00 best_ppl=5.23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "seq2seq10 테스트"
      ],
      "metadata": {
        "id": "GTA2a-nr1Jsi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 5 /content/drive/MyDrive/nmt_fold/data1/combined.shuf.test.tok.bpe.en | python /content/drive/MyDrive/nmt_fold/simple-nmt/translate.py --model_fn /content/drive/MyDrive/nmt_fold/data1/models1.seq2seq10.pth --gpu_id -1 --batch_size 2 --beam_size 1 | python /content/drive/MyDrive/nmt_fold/code1/detokenizer.py\n",
        "print(\"==============\")\n",
        "!head -n 5 /content/drive/MyDrive/nmt_fold/data1/combined.shuf.test.ko"
      ],
      "metadata": {
        "id": "axZGTBiU1HXi",
        "outputId": "342ffc25-4b79-4271-eaec-52d62ba3cd8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "개방된 공기공간 내 인근 건물이 조성한 자체적인 집집이지만 흡연자들이 쏟아내는 무분별한 담배연기 연기가 시민들이 몰려들어 시민들이 몰려들고 있는 공원 밖의 보행자길으로 확대됐다.\n",
            "오리지널의 생리대 라인과는 차이가 있나요?\n",
            "\"이 가을이 다가오기 전에도 창덕궁과 하정당을 찾아, 이곳에 반사된 시대의 아픔을 느끼는 것도 좋다.\"\n",
            "\"1차 1차는 전혀 반응하지 않고, 약물의 초기 효과를 보여주는 환자의 1분의 1가량이 약한 강도가 되어 약이 안 나온다.\"\n",
            "그는 “경제성을 배제하고 배제할 수 있는 핵심은 기업 소유구조”라며 “기업 소유구조는 기업 소유구조”라며 향후 사업모델이 ‘참여기업’이 될 것이라고 전망했다.\n",
            "==============\n",
            "이곳은 공개공지내 인근 건물에서 자체 조성한 공원이지만 흡연자들이 뿜어대는 무분별한 담배연기가 공원외부 보행로까지 확대되어 시민들의 눈살을 찌뿌리게 한 곳이다.\n",
            "미디헴팬티는 기존에 있는 속옷과 다른 것입니까?\n",
            "이 가을이 다 가기 전에 창덕궁과 희정당을 찾아 그 속에 담긴 시대적인 아픔을 한번 느껴보는 것도 좋을 것 같다.\n",
            "\"약 3분의 1은 전혀 반응을 보이지 않으며, 투약 초기 효과를 보이던 환자들도 약 3분의 1은 내성이 생기는지 점차 약발이 듣지 않게 된다.\"\n",
            "\"그는 “경제적 포용과 배제를 가르는 관건은 기업의 소유구조”라며, 소유권이 소수에게 집중된 기존 경제모델이 힘을 잃어가는 반면 미래 비즈니스 모델은 ‘참여적 기업’이 될 것이라고 전망했다.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "transformer20 테스트"
      ],
      "metadata": {
        "id": "SSAM2Abk1A7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 5 /content/drive/MyDrive/nmt_fold/data1/combined.shuf.test.tok.bpe.en | python /content/drive/MyDrive/nmt_fold/simple-nmt/translate.py --model_fn /content/drive/MyDrive/nmt_fold/data1/models1.transformer20.pth --gpu_id -1 --batch_size 2 --beam_size 1 | python /content/drive/MyDrive/nmt_fold/code1/detokenizer.py\n",
        "print(\"==============\")\n",
        "!head -n 5 /content/drive/MyDrive/nmt_fold/data1/combined.shuf.test.ko"
      ],
      "metadata": {
        "id": "nH1S-8GTwDI0",
        "outputId": "9f742655-6a7c-4a30-b5f5-6a97278b0ec4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"공개된 공중내 건물 내에 인근 건물들이 만든 셀프공원이지만, 시민들이 몰던 공원 밖 보행길까지 확대해 시민들이 몰리고 있는 것이다.\"\n",
            "그가 원하는 옷라인과 다른 중간품은 있나요?\n",
            "가을이 되기 전에도 창덕궁과 하숙당을 찾아 현장에 반영된 시대의 고통을 느껴보는 것도 좋다.\n",
            "\"약 1,3명의 환자가 모두 반응하지 않고, 약을 먹는 초기 효과를 보인 환자 중 3분의 1 정도가 적어 약이 킥이 나지 않는다.\"\n",
            "그는 “경제적으로는 기업의 소유권이자 배제되는 핵심이 기업 소유구조”라며 “미래 비즈니스 모델이 도약할 것”이라고 전망했다.\n",
            "==============\n",
            "이곳은 공개공지내 인근 건물에서 자체 조성한 공원이지만 흡연자들이 뿜어대는 무분별한 담배연기가 공원외부 보행로까지 확대되어 시민들의 눈살을 찌뿌리게 한 곳이다.\n",
            "미디헴팬티는 기존에 있는 속옷과 다른 것입니까?\n",
            "이 가을이 다 가기 전에 창덕궁과 희정당을 찾아 그 속에 담긴 시대적인 아픔을 한번 느껴보는 것도 좋을 것 같다.\n",
            "\"약 3분의 1은 전혀 반응을 보이지 않으며, 투약 초기 효과를 보이던 환자들도 약 3분의 1은 내성이 생기는지 점차 약발이 듣지 않게 된다.\"\n",
            "\"그는 “경제적 포용과 배제를 가르는 관건은 기업의 소유구조”라며, 소유권이 소수에게 집중된 기존 경제모델이 힘을 잃어가는 반면 미래 비즈니스 모델은 ‘참여적 기업’이 될 것이라고 전망했다.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cn9e5kpY1a7S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}