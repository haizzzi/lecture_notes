{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "https://github.com/haizzzi/simple-nmt-simplied/blob/main/NMT_Exe.ipynb",
      "authorship_tag": "ABX9TyOgWRTPNbe+sS9sR99WnciY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haizzzi/simple-nmt-simplied/blob/main/NMT_Exe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "제일먼저 구글드라이버에 nmt-fold에 code1과 data1이 설치되도록 자신의 구글드라이버에 제공되는 zip 파일을 압축 해제해서 upload 하세요. 그리고 좌측 mount 버튼을 누르세요. 그러면 구글 드라이브 마운트 위치 \"/content/drive/MyDrive/\"에 마운트가 되어서 /content/drive/MyDrive/nmt_fold/data1/ 형태로 접근이 됩니다."
      ],
      "metadata": {
        "id": "hXKDzevwElOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "!pip install torch_optimizer\n",
        "!pip install pytorch-ignite"
      ],
      "metadata": {
        "id": "CoHvjUX6mkcZ",
        "outputId": "64f6afcb-821c-4ae1-c87c-7fb423124c06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-ignite\n",
            "  Downloading pytorch_ignite-0.5.2-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: torch<3,>=1.3 in /usr/local/lib/python3.11/dist-packages (from pytorch-ignite) (2.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pytorch-ignite) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=1.3->pytorch-ignite) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=1.3->pytorch-ignite) (3.0.2)\n",
            "Downloading pytorch_ignite-0.5.2-py3-none-any.whl (343 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/343.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m343.2/343.2 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytorch-ignite\n",
            "Successfully installed pytorch-ignite-0.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)\n"
      ],
      "metadata": {
        "id": "MVaPUSLB8YNu",
        "outputId": "77e70177-2010-4873-8bf8-4bf563e48cbf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0+cu124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "seq2seq 학습 시키는 코드"
      ],
      "metadata": {
        "id": "Q9Btzoq1HiXo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAUvswrLkv9m",
        "outputId": "2799b596-8f48-4767-8597-1b2f921e7a5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{   'batch_size': 160,\n",
            "    'dropout': 0.2,\n",
            "    'gpu_id': 0,\n",
            "    'hidden_size': 768,\n",
            "    'init_epoch': 1,\n",
            "    'iteration_per_update': 2,\n",
            "    'lang': 'enko',\n",
            "    'lr': 0.001,\n",
            "    'lr_decay_start': 10,\n",
            "    'lr_gamma': 0.5,\n",
            "    'lr_step': 0,\n",
            "    'max_grad_norm': 100000000.0,\n",
            "    'max_length': 64,\n",
            "    'model_fn': '/content/drive/MyDrive/nmt_fold/data1/models1.seq2seq.pth',\n",
            "    'n_epochs': 30,\n",
            "    'n_layers': 4,\n",
            "    'n_splits': 8,\n",
            "    'off_autocast': False,\n",
            "    'rl_lr': 0.01,\n",
            "    'rl_n_epochs': 10,\n",
            "    'rl_n_gram': 6,\n",
            "    'rl_n_samples': 1,\n",
            "    'rl_reward': 'gleu',\n",
            "    'train': '/content/drive/MyDrive/nmt_fold/data1/combined.shuf.train.tok.bpe',\n",
            "    'use_adam': True,\n",
            "    'use_radam': False,\n",
            "    'use_transformer': False,\n",
            "    'valid': '/content/drive/MyDrive/nmt_fold/data1/combined.shuf.valid.tok.bpe',\n",
            "    'verbose': 2,\n",
            "    'word_vec_size': 512}\n",
            "Seq2Seq(\n",
            "  (emb_src): Embedding(20264, 512)\n",
            "  (emb_dec): Embedding(22291, 512)\n",
            "  (encoder): Encoder(\n",
            "    (rnn): LSTM(512, 384, num_layers=4, batch_first=True, dropout=0.2, bidirectional=True)\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (rnn): LSTM(1280, 768, num_layers=4, batch_first=True, dropout=0.2)\n",
            "  )\n",
            "  (attn): Attention(\n",
            "    (linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "    (softmax): Softmax(dim=-1)\n",
            "  )\n",
            "  (concat): Linear(in_features=1536, out_features=768, bias=True)\n",
            "  (tanh): Tanh()\n",
            "  (generator): Generator(\n",
            "    (output): Linear(in_features=768, out_features=22291, bias=True)\n",
            "    (softmax): LogSoftmax(dim=-1)\n",
            "  )\n",
            ")\n",
            "NLLLoss()\n",
            "Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    weight_decay: 0\n",
            ")\n",
            "/content/drive/MyDrive/nmt_fold/simple-nmt/simple_nmt/trainer.py:34: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = GradScaler()\n",
            "/content/drive/MyDrive/nmt_fold/simple-nmt/simple_nmt/trainer.py:58: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(not engine.config.off_autocast):\n",
            "Epoch [1/30]:   1%|     | 43/6779 [00:26<1:07:38,  1.66it/s, loss=8.23, ppl=8.08e+3, |param|=4.67e+3, |g_param|=7.31e+5]Engine run is terminating due to exception: \n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/nmt_fold/simple-nmt/train.py\", line 361, in <module>\n",
            "    main(config)\n",
            "  File \"/content/drive/MyDrive/nmt_fold/simple-nmt/train.py\", line 331, in main\n",
            "    mle_trainer.train(\n",
            "  File \"/content/drive/MyDrive/nmt_fold/simple-nmt/simple_nmt/trainer.py\", line 311, in train\n",
            "    train_engine.run(train_loader, max_epochs=n_epochs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ignite/engine/engine.py\", line 905, in run\n",
            "    return self._internal_run()\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ignite/engine/engine.py\", line 948, in _internal_run\n",
            "    return next(self._internal_run_generator)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ignite/engine/engine.py\", line 1023, in _internal_run_as_gen\n",
            "    self._handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ignite/engine/engine.py\", line 660, in _handle_exception\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ignite/engine/engine.py\", line 972, in _internal_run_as_gen\n",
            "    epoch_time_taken += yield from self._run_once_on_dataset_as_gen()\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ignite/engine/engine.py\", line 1110, in _run_once_on_dataset_as_gen\n",
            "    self.state.output = self._process_function(self, self.state.batch)\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/nmt_fold/simple-nmt/simple_nmt/trainer.py\", line 62, in train\n",
            "    y_hat = engine.model(x, mini_batch.tgt[0][:, :-1])\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/nmt_fold/simple-nmt/simple_nmt/models/seq2seq.py\", line 265, in forward\n",
            "    h_src, h_0_tgt = self.encoder((emb_src, x_length))\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/nmt_fold/simple-nmt/simple_nmt/models/seq2seq.py\", line 65, in forward\n",
            "    x = pack(x, lengths.tolist(), batch_first=True)\n",
            "                ^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "!python /content/drive/MyDrive/nmt_fold/simple-nmt/train.py --train /content/drive/MyDrive/nmt_fold/data1/combined.shuf.train.tok.bpe --valid /content/drive/MyDrive/nmt_fold/data1/combined.shuf.valid.tok.bpe --lang enko --gpu_id 0 --batch_size 160 --n_epochs 30 --max_length 64 --dropout .2 --word_vec_size 512 --hidden_size 768 --n_layers 4 --max_grad_norm 1e+8 --iteration_per_update 2 --lr 1e-3 --lr_step 0 --use_adam --model_fn /content/drive/MyDrive/nmt_fold/data1/models1.seq2seq.pth"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "trasformer학습 시키는 코드"
      ],
      "metadata": {
        "id": "VeqqzmE5He3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/nmt_fold/simple-nmt/train.py --train /content/drive/MyDrive/nmt_fold/data1/combined.shuf.train.tok.bpe --valid /content/drive/MyDrive/nmt_fold/data1/combined.shuf.valid.tok.bpe --lang enko --gpu_id 0 --batch_size 128 --n_epochs 30 --max_length 64 --dropout .2 --hidden_size 768 --n_layers 4 --max_grad_norm 1e+8 --iteration_per_update 32 --lr 1e-3 --lr_step 0 --use_adam --rl_n_epochs 0 --use_transformer --model_fn /content/drive/MyDrive/nmt_fold/data1/models1.transformer.pth"
      ],
      "metadata": {
        "id": "_PfN0bpFHeJp",
        "outputId": "d259eebe-2881-4b68-bfb0-05e5eb7c64e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{   'batch_size': 128,\n",
            "    'dropout': 0.2,\n",
            "    'gpu_id': 0,\n",
            "    'hidden_size': 768,\n",
            "    'init_epoch': 1,\n",
            "    'iteration_per_update': 32,\n",
            "    'lang': 'enko',\n",
            "    'lr': 0.001,\n",
            "    'lr_decay_start': 10,\n",
            "    'lr_gamma': 0.5,\n",
            "    'lr_step': 0,\n",
            "    'max_grad_norm': 100000000.0,\n",
            "    'max_length': 64,\n",
            "    'model_fn': '/content/drive/MyDrive/nmt_fold/data1/models1.transformer.pth',\n",
            "    'n_epochs': 30,\n",
            "    'n_layers': 4,\n",
            "    'n_splits': 8,\n",
            "    'off_autocast': False,\n",
            "    'rl_lr': 0.01,\n",
            "    'rl_n_epochs': 0,\n",
            "    'rl_n_gram': 6,\n",
            "    'rl_n_samples': 1,\n",
            "    'rl_reward': 'gleu',\n",
            "    'train': '/content/drive/MyDrive/nmt_fold/data1/combined.shuf.train.tok.bpe',\n",
            "    'use_adam': True,\n",
            "    'use_radam': False,\n",
            "    'use_transformer': True,\n",
            "    'valid': '/content/drive/MyDrive/nmt_fold/data1/combined.shuf.valid.tok.bpe',\n",
            "    'verbose': 2,\n",
            "    'word_vec_size': 512}\n",
            "Transformer(\n",
            "  (emb_enc): Embedding(20264, 768)\n",
            "  (emb_dec): Embedding(22291, 768)\n",
            "  (emb_dropout): Dropout(p=0.2, inplace=False)\n",
            "  (encoder): MySequential(\n",
            "    (0): EncoderBlock(\n",
            "      (attn): MultiHead(\n",
            "        (Q_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (K_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (V_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (attn): Attention(\n",
            "          (softmax): Softmax(dim=-1)\n",
            "        )\n",
            "      )\n",
            "      (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (attn_dropout): Dropout(p=0.2, inplace=False)\n",
            "      (fc): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (1): ReLU()\n",
            "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "      )\n",
            "      (fc_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (fc_dropout): Dropout(p=0.2, inplace=False)\n",
            "    )\n",
            "    (1): EncoderBlock(\n",
            "      (attn): MultiHead(\n",
            "        (Q_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (K_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (V_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (attn): Attention(\n",
            "          (softmax): Softmax(dim=-1)\n",
            "        )\n",
            "      )\n",
            "      (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (attn_dropout): Dropout(p=0.2, inplace=False)\n",
            "      (fc): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (1): ReLU()\n",
            "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "      )\n",
            "      (fc_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (fc_dropout): Dropout(p=0.2, inplace=False)\n",
            "    )\n",
            "    (2): EncoderBlock(\n",
            "      (attn): MultiHead(\n",
            "        (Q_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (K_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (V_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (attn): Attention(\n",
            "          (softmax): Softmax(dim=-1)\n",
            "        )\n",
            "      )\n",
            "      (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (attn_dropout): Dropout(p=0.2, inplace=False)\n",
            "      (fc): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (1): ReLU()\n",
            "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "      )\n",
            "      (fc_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (fc_dropout): Dropout(p=0.2, inplace=False)\n",
            "    )\n",
            "    (3): EncoderBlock(\n",
            "      (attn): MultiHead(\n",
            "        (Q_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (K_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (V_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (attn): Attention(\n",
            "          (softmax): Softmax(dim=-1)\n",
            "        )\n",
            "      )\n",
            "      (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (attn_dropout): Dropout(p=0.2, inplace=False)\n",
            "      (fc): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (1): ReLU()\n",
            "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "      )\n",
            "      (fc_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (fc_dropout): Dropout(p=0.2, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (decoder): MySequential(\n",
            "    (0): DecoderBlock(\n",
            "      (masked_attn): MultiHead(\n",
            "        (Q_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (K_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (V_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (attn): Attention(\n",
            "          (softmax): Softmax(dim=-1)\n",
            "        )\n",
            "      )\n",
            "      (masked_attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (masked_attn_dropout): Dropout(p=0.2, inplace=False)\n",
            "      (attn): MultiHead(\n",
            "        (Q_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (K_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (V_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (attn): Attention(\n",
            "          (softmax): Softmax(dim=-1)\n",
            "        )\n",
            "      )\n",
            "      (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (attn_dropout): Dropout(p=0.2, inplace=False)\n",
            "      (fc): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (1): ReLU()\n",
            "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "      )\n",
            "      (fc_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (fc_dropout): Dropout(p=0.2, inplace=False)\n",
            "    )\n",
            "    (1): DecoderBlock(\n",
            "      (masked_attn): MultiHead(\n",
            "        (Q_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (K_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (V_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (attn): Attention(\n",
            "          (softmax): Softmax(dim=-1)\n",
            "        )\n",
            "      )\n",
            "      (masked_attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (masked_attn_dropout): Dropout(p=0.2, inplace=False)\n",
            "      (attn): MultiHead(\n",
            "        (Q_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (K_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (V_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (attn): Attention(\n",
            "          (softmax): Softmax(dim=-1)\n",
            "        )\n",
            "      )\n",
            "      (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (attn_dropout): Dropout(p=0.2, inplace=False)\n",
            "      (fc): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (1): ReLU()\n",
            "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "      )\n",
            "      (fc_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (fc_dropout): Dropout(p=0.2, inplace=False)\n",
            "    )\n",
            "    (2): DecoderBlock(\n",
            "      (masked_attn): MultiHead(\n",
            "        (Q_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (K_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (V_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (attn): Attention(\n",
            "          (softmax): Softmax(dim=-1)\n",
            "        )\n",
            "      )\n",
            "      (masked_attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (masked_attn_dropout): Dropout(p=0.2, inplace=False)\n",
            "      (attn): MultiHead(\n",
            "        (Q_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (K_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (V_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (attn): Attention(\n",
            "          (softmax): Softmax(dim=-1)\n",
            "        )\n",
            "      )\n",
            "      (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (attn_dropout): Dropout(p=0.2, inplace=False)\n",
            "      (fc): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (1): ReLU()\n",
            "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "      )\n",
            "      (fc_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (fc_dropout): Dropout(p=0.2, inplace=False)\n",
            "    )\n",
            "    (3): DecoderBlock(\n",
            "      (masked_attn): MultiHead(\n",
            "        (Q_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (K_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (V_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (attn): Attention(\n",
            "          (softmax): Softmax(dim=-1)\n",
            "        )\n",
            "      )\n",
            "      (masked_attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (masked_attn_dropout): Dropout(p=0.2, inplace=False)\n",
            "      (attn): MultiHead(\n",
            "        (Q_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (K_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (V_linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (linear): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (attn): Attention(\n",
            "          (softmax): Softmax(dim=-1)\n",
            "        )\n",
            "      )\n",
            "      (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (attn_dropout): Dropout(p=0.2, inplace=False)\n",
            "      (fc): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (1): ReLU()\n",
            "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "      )\n",
            "      (fc_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (fc_dropout): Dropout(p=0.2, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (generator): Sequential(\n",
            "    (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "    (1): Linear(in_features=768, out_features=22291, bias=True)\n",
            "    (2): LogSoftmax(dim=-1)\n",
            "  )\n",
            ")\n",
            "NLLLoss()\n",
            "Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.98)\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    weight_decay: 0\n",
            ")\n",
            "/content/drive/MyDrive/nmt_fold/simple-nmt/simple_nmt/trainer.py:34: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = GradScaler()\n",
            "/content/drive/MyDrive/nmt_fold/simple-nmt/simple_nmt/trainer.py:58: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(not engine.config.off_autocast):\n",
            "Epoch [1/30]:   4%|▍         | 362/8474 [02:05<48:35,  2.78it/s, loss=6.56, ppl=737, |param|=5.72e+3, |g_param|=8.61e+5]Engine run is terminating due to exception: \n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/nmt_fold/simple-nmt/train.py\", line 361, in <module>\n",
            "    main(config)\n",
            "  File \"/content/drive/MyDrive/nmt_fold/simple-nmt/train.py\", line 331, in main\n",
            "    mle_trainer.train(\n",
            "  File \"/content/drive/MyDrive/nmt_fold/simple-nmt/simple_nmt/trainer.py\", line 311, in train\n",
            "    train_engine.run(train_loader, max_epochs=n_epochs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ignite/engine/engine.py\", line 905, in run\n",
            "    return self._internal_run()\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ignite/engine/engine.py\", line 948, in _internal_run\n",
            "    return next(self._internal_run_generator)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ignite/engine/engine.py\", line 1023, in _internal_run_as_gen\n",
            "    self._handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ignite/engine/engine.py\", line 660, in _handle_exception\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ignite/engine/engine.py\", line 972, in _internal_run_as_gen\n",
            "    epoch_time_taken += yield from self._run_once_on_dataset_as_gen()\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ignite/engine/engine.py\", line 1110, in _run_once_on_dataset_as_gen\n",
            "    self.state.output = self._process_function(self, self.state.batch)\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/nmt_fold/simple-nmt/simple_nmt/trainer.py\", line 77, in train\n",
            "    p_norm = float(get_parameter_norm(engine.model.parameters()))\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/nmt_fold/simple-nmt/simple_nmt/utils.py\", line 27, in get_parameter_norm\n",
            "    total_norm += (p.data**norm_type).sum()\n",
            "    ^^^^^^^^^^\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python continue_train.py --load_fn /content/drive/MyDrive/nmt_fold/data1/models1.transformer10.pth --model_fn /content/drive/MyDrive/nmt_fold/data1/models1.transformer.pth --init_epoch 11 --n_epochs 20 --use_transformer"
      ],
      "metadata": {
        "id": "POEQwe--Vpz3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}