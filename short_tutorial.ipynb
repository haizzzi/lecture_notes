{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haizzzi/lecture_notes/blob/main/short_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UZaP9z4_Uw5"
      },
      "source": [
        "# Short Tutorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0jJcRyE_Uw7"
      },
      "source": [
        "## Tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VtxetDGK_Uw8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "x = torch.Tensor(2, 2)\n",
        "x = torch.Tensor([[1, 2], [3, 4]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_Cd5crSW_Uw9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "x = [[1, 2], [3, 4]]\n",
        "x = np.array(x)\n",
        "x = torch.from_numpy(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhHa6_-U_Uw9"
      },
      "source": [
        "$$x=\\begin{bmatrix}\n",
        "1, 2 \\\\\n",
        "3, 4\n",
        "\\end{bmatrix}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewEU74SE_Uw9"
      },
      "source": [
        "## Autograd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-UWr1maC_Uw-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "x = torch.FloatTensor(2, 2)\n",
        "y = torch.FloatTensor(2, 2)\n",
        "y.requires_grad_(True)\n",
        "\n",
        "z = (x + y) + torch.FloatTensor(2, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "86RiqrMX_Uw-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "x = torch.FloatTensor(2, 2)\n",
        "y = torch.FloatTensor(2, 2)\n",
        "y.requires_grad_(True)\n",
        "\n",
        "with torch.no_grad():\n",
        "    z = (x + y) + torch.FloatTensor(2, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8guh6vy_Uw-"
      },
      "source": [
        "## Feed-forward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qu79k-k-_Uw_"
      },
      "source": [
        "$$\\begin{gathered}\n",
        "y = xW+ b \\\\\n",
        "\\text{where }x\\in\\mathbb{R}^{M\\times N},W\\in\\mathbb{R}^{N\\times P}\\text{ and }b\\in\\mathbb{R}^P. \\\\\n",
        "\\text{Thus, }y\\in\\mathbb{R}^{M\\times P}.\n",
        "\\end{gathered}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dLyUrKp_Uw_"
      },
      "source": [
        "$$\\begin{aligned}\n",
        "y&=f(x; \\theta)\\text{ where }\\theta=\\{W, b\\}\n",
        "\\end{aligned}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2DUlUVDw_Uw_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def linear(x, W, b):\n",
        "    y = torch.mm(x, W) + b\n",
        "\n",
        "    return y\n",
        "\n",
        "x = torch.FloatTensor(16, 10)\n",
        "W = torch.FloatTensor(10, 5)\n",
        "b = torch.FloatTensor(5)\n",
        "\n",
        "y = linear(x, W, b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Epb2veKq_UxA"
      },
      "source": [
        "## nn.Module\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zaioKoS_UxA",
        "outputId": "9a55f632-7243-4efd-b37b-e2042c78f212"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-4.9250e+06, -3.6932e-09, -4.9546e+06,  1.9619e-09,  5.9238e-09],\n",
            "        [ 2.8393e-01, -1.0495e-39,  1.2314e-01, -4.9767e-41, -3.8089e-40],\n",
            "        [ 2.8393e-01, -1.0494e-39,  1.2314e-01, -4.9666e-41, -3.8072e-40],\n",
            "        [ 2.8393e-01, -1.0494e-39,  1.2314e-01, -4.9511e-41, -3.8040e-40],\n",
            "        [ 2.8393e-01, -1.0496e-39,  1.2314e-01, -4.9710e-41, -3.8069e-40],\n",
            "        [ 2.8393e-01, -1.0496e-39,  1.2314e-01, -4.9818e-41, -3.8066e-40],\n",
            "        [ 2.8393e-01, -1.0493e-39,  1.2314e-01, -4.9525e-41, -3.8048e-40],\n",
            "        [ 2.8393e-01, -1.0496e-39,  1.2314e-01, -4.9745e-41, -3.8076e-40],\n",
            "        [ 2.8393e-01, -1.0494e-39,  1.2314e-01, -4.9642e-41, -3.8074e-40],\n",
            "        [ 2.8393e-01, -1.0496e-39,  1.2314e-01, -4.9733e-41, -3.8073e-40],\n",
            "        [ 2.8393e-01, -1.0495e-39,  1.2314e-01, -4.9746e-41, -3.8058e-40],\n",
            "        [ 2.8393e-01, -1.0496e-39,  1.2314e-01, -4.9696e-41, -3.8070e-40],\n",
            "        [ 2.8393e-01, -1.0492e-39,  1.2314e-01, -4.9696e-41, -3.8051e-40],\n",
            "        [ 2.8393e-01, -1.0493e-39,  1.2314e-01, -4.9718e-41, -3.8084e-40],\n",
            "        [ 2.8393e-01, -1.0495e-39,  1.2314e-01, -4.9766e-41, -3.8085e-40],\n",
            "        [ 2.8393e-01, -1.0496e-39,  1.2314e-01, -4.9701e-41, -3.8067e-40]])\n",
            "[]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MyLinear(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.W = torch.FloatTensor(input_size, output_size)\n",
        "        self.b = torch.FloatTensor(output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = torch.mm(x, self.W) + self.b\n",
        "\n",
        "        return y\n",
        "x = torch.FloatTensor(16, 10)\n",
        "linear = MyLinear(10, 5)\n",
        "y = linear(x)\n",
        "print(y)\n",
        "\n",
        "params = [p.size() for p in linear.parameters()]\n",
        "print(params)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "nn.Module 안에 선언된 텐서가 자동으로 파라미터로 등록되려면 반드시 nn.Parameter 로 감싸야 합니다.\n",
        "\n",
        "지금 코드에서는\n",
        "\n",
        "self.W = torch.FloatTensor(input_size, output_size)\n",
        "self.b = torch.FloatTensor(output_size)\n",
        "\n",
        "\n",
        "로 선언했기 때문에, linear.parameters()에 포함되지 않습니다. 따라서 params는 빈 리스트가 됩니다.\n",
        "\n",
        "왜 그런가?\n",
        "\n",
        "nn.Module은 nn.Parameter 타입인 속성만 학습 가능한 파라미터로 인식합니다.\n",
        "\n",
        "그냥 torch.FloatTensor(...)를 넣으면 일반 텐서일 뿐, requires_grad=True가 아니고 optimizer가 업데이트도 안 합니다."
      ],
      "metadata": {
        "id": "Hm2xiZGyJO4h"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xI06NT1M_UxB"
      },
      "source": [
        "참고: http://pytorch.org/docs/master/nn.html?highlight=parameter#parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrXvaWw7_UxB",
        "outputId": "1765e738-e006-4ee0-dedf-bdc44928376e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[torch.Size([10, 5]), torch.Size([5])]\n"
          ]
        }
      ],
      "source": [
        "class MyLinear(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(MyLinear, self).__init__()\n",
        "\n",
        "        self.W = nn.Parameter(torch.FloatTensor(input_size, output_size), requires_grad=True)\n",
        "        self.b = nn.Parameter(torch.FloatTensor(output_size), requires_grad=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = torch.mm(x, self.W) + self.b\n",
        "\n",
        "        return y\n",
        "linear = MyLinear(10, 5)\n",
        "params = [p.size() for p in linear.parameters()]\n",
        "print(params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "RbSa8ZJ3_UxC"
      },
      "outputs": [],
      "source": [
        "class MyLinear(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(MyLinear, self).__init__()\n",
        "\n",
        "        self.linear = nn.Linear(input_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.linear(x)\n",
        "\n",
        "        return y\n",
        "\n",
        "linear = MyLinear(10, 5)\n",
        "print(linear)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vs2Oi2U6_UxC"
      },
      "source": [
        "## Backward (Back-propagation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYV1Sb_i_UxC",
        "outputId": "b03a2f90-7b13-4f76-d9fa-d961e0cf6d59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(inf, grad_fn=<PowBackward0>)\n"
          ]
        }
      ],
      "source": [
        "objective = 100\n",
        "\n",
        "x = torch.FloatTensor(16, 10)\n",
        "linear = MyLinear(10, 5)\n",
        "y = linear(x)\n",
        "loss = (objective - y.sum())**2\n",
        "\n",
        "loss.backward()\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCfAnTBE_UxD"
      },
      "source": [
        "## train() and eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhyVefbh_UxD",
        "outputId": "2c18fb2a-6e84-4fba-dd5b-e9dcad8dc569"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MyLinear(\n",
              "  (linear): Linear(in_features=10, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "# Training...\n",
        "linear.eval()\n",
        "# Do some inference process.\n",
        "linear.train()\n",
        "# Restart training, again."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biPl6c7y_UxD"
      },
      "source": [
        "## Linear regression example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTUciN1w_UxD"
      },
      "source": [
        "$$\\mathcal{L}_{\\text{MSE}}(\\hat{y}, y)=\\frac{1}{N}\\sum^N_{i=1}{(\\hat{y}_i - y_i)^2}$$\n",
        "\n",
        "$$\\begin{gathered}\n",
        "y=f(x_1, x_2, x_3) = 3x_1 + x_2 - 2x_3 \\\\\n",
        "\\hat{y}=\\tilde{f}(x_1,x_2,x_3;\\theta) \\\\\n",
        "\\hat{\\theta}=\\underset{\\theta\\in\\Theta}{\\text{argmin }}\\mathcal{L}(\\hat{y},y)\n",
        "\\end{gathered}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcXjVC9o_UxE",
        "outputId": "0bdc094a-bec9-4de9-df92-a7492bc1f6f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MyModel(\n",
            "  (linear): Linear(in_features=3, out_features=1, bias=True)\n",
            ")\n",
            "tensor(0.8493) tensor([0.9000]) tensor(0.7928)\n",
            "tensor(0.4929) tensor([0.9000]) tensor(0.8287)\n",
            "tensor(0.3552) tensor([0.9000]) tensor(0.8537)\n",
            "tensor(0.2655) tensor([0.9000]) tensor(0.8805)\n",
            "tensor(0.1866) tensor([0.9000]) tensor(0.8978)\n",
            "tensor(0.1347) tensor([0.9000]) tensor(0.9074)\n",
            "tensor(0.0951) tensor([0.9000]) tensor(0.9176)\n",
            "tensor(0.0691) tensor([0.9000]) tensor(0.9234)\n",
            "tensor(0.0508) tensor([0.9000]) tensor(0.9354)\n",
            "tensor(0.0363) tensor([0.9000]) tensor(0.9334)\n",
            "tensor(0.0260) tensor([0.9000]) tensor(0.9353)\n",
            "tensor(0.0193) tensor([0.9000]) tensor(0.9397)\n",
            "tensor(0.0141) tensor([0.9000]) tensor(0.9386)\n",
            "tensor(0.0100) tensor([0.9000]) tensor(0.9379)\n",
            "tensor(0.0074) tensor([0.9000]) tensor(0.9374)\n",
            "tensor(0.0055) tensor([0.9000]) tensor(0.9371)\n",
            "tensor(0.0040) tensor([0.9000]) tensor(0.9346)\n",
            "tensor(0.0030) tensor([0.9000]) tensor(0.9340)\n",
            "tensor(0.0023) tensor([0.9000]) tensor(0.9319)\n",
            "tensor(0.0017) tensor([0.9000]) tensor(0.9303)\n",
            "tensor(0.0013) tensor([0.9000]) tensor(0.9286)\n",
            "tensor(0.0009) tensor([0.9000]) tensor(0.9272)\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(MyModel, self).__init__()\n",
        "\n",
        "        self.linear = nn.Linear(input_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.linear(x)\n",
        "\n",
        "        return y\n",
        "\n",
        "def ground_truth(x):\n",
        "    y = 3 * x[:, 0] + x[:, 1] - 2 * x[:, 2]\n",
        "    return y.unsqueeze(1)  # (B,1)\n",
        "\n",
        "def train(model, x, y, optim):\n",
        "    # initialize gradients in all parameters in module.\n",
        "    optim.zero_grad()\n",
        "\n",
        "    # feed-forward\n",
        "    y_hat = model(x)\n",
        "    # get error between answer and inferenced.\n",
        "    loss = ((y - y_hat)**2).mean()\n",
        "\n",
        "    # back-propagation\n",
        "    loss.backward()\n",
        "\n",
        "    # one-step of gradient descent\n",
        "    optim.step()\n",
        "\n",
        "    return loss.data\n",
        "\n",
        "batch_size = 64\n",
        "n_epochs = 100\n",
        "n_iter = 100\n",
        "\n",
        "model = MyModel(3, 1)\n",
        "optim = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9)\n",
        "print(model)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    avg_loss = 0\n",
        "\n",
        "    for i in range(n_iter):\n",
        "        x = torch.rand(batch_size, 3)\n",
        "        y = ground_truth(x)  # .data 제거\n",
        "\n",
        "        loss = train(model, x, y, optim)\n",
        "\n",
        "        avg_loss += loss\n",
        "    avg_loss = avg_loss / n_iter\n",
        "\n",
        "    # simple test sample to check the network.\n",
        "    x_valid = torch.FloatTensor([[.3, .2, .1]])\n",
        "    y_valid = ground_truth(x_valid.data)\n",
        "\n",
        "    model.eval()\n",
        "    y_hat = model(x_valid)\n",
        "    model.train()\n",
        "\n",
        "    print(avg_loss, y_valid.data[0], y_hat.data[0, 0])\n",
        "\n",
        "    if avg_loss < .001: # finish the training if the loss is smaller than .001.\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mju2BVYY_UxF"
      },
      "source": [
        "## Use GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XE_D6eyu_UxG",
        "outputId": "3cd73a76-49ba-4048-b410-694c29e05532"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.383076980113983 0.9000000357627869 0.506269633769989\n",
            "0.8366517925262451 0.9000000357627869 0.5701144337654114\n",
            "0.5775570678710937 0.9000000357627869 0.6318530440330505\n",
            "0.4101344475150108 0.9000000357627869 0.687428891658783\n",
            "0.28684134036302567 0.9000000357627869 0.7203226685523987\n",
            "0.20550175063312054 0.9000000357627869 0.7528856992721558\n",
            "0.1489201507717371 0.9000000357627869 0.7700124382972717\n",
            "0.10575040176510811 0.9000000357627869 0.7898146510124207\n",
            "0.07683575615286826 0.9000000357627869 0.8155608177185059\n",
            "0.05435170091688633 0.9000000357627869 0.8259813785552979\n",
            "0.03953284310176969 0.9000000357627869 0.8461651802062988\n",
            "0.028161329794675113 0.9000000357627869 0.8532272577285767\n",
            "0.019513621255755426 0.9000000357627869 0.8657689094543457\n",
            "0.014106674948707223 0.9000000357627869 0.8707885146141052\n",
            "0.010128388414159417 0.9000000357627869 0.8753489255905151\n",
            "0.00728272351436317 0.9000000357627869 0.8798295259475708\n",
            "0.005111966333352029 0.9000000357627869 0.8857614398002625\n",
            "0.003734011105261743 0.9000000357627869 0.888425350189209\n",
            "0.0025925330212339757 0.9000000357627869 0.8917326331138611\n",
            "0.0018802441528532653 0.9000000357627869 0.8936983942985535\n",
            "0.0013700204982887954 0.9000000357627869 0.8965758681297302\n",
            "0.00096908048260957 0.9000000357627869 0.8976778388023376\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# ----- 공통 설정 -----\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(input_size, output_size)\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "def ground_truth(x):\n",
        "    # x: (..., 3)  -> y: (..., 1)\n",
        "    y = 3 * x[:, 0] + x[:, 1] - 2 * x[:, 2]\n",
        "    return y.unsqueeze(1)\n",
        "\n",
        "def train(model, x, y, optim):\n",
        "    optim.zero_grad()\n",
        "    y_hat = model(x)\n",
        "    loss = ((y - y_hat) ** 2).mean()   # 평균으로 변경(권장)\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    return loss.item()\n",
        "\n",
        "# ----- 모델/옵티마이저 -----\n",
        "model = MyModel(3, 1).to(device)\n",
        "optim = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9)\n",
        "\n",
        "# ----- 학습 루프 -----\n",
        "batch_size = 64\n",
        "n_epochs = 100\n",
        "n_iter = 100\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    avg_loss = 0.0\n",
        "\n",
        "    for i in range(n_iter):\n",
        "        # 입력과 타깃을 '처음부터' 같은 device로 생성\n",
        "        x = torch.rand(batch_size, 3, device=device)\n",
        "        y = ground_truth(x)  # x가 CUDA면 y도 CUDA로 나옵니다.\n",
        "\n",
        "        loss = train(model, x, y, optim)\n",
        "        avg_loss += loss\n",
        "\n",
        "    avg_loss /= n_iter\n",
        "\n",
        "    # 검증 샘플도 같은 device로\n",
        "    x_valid = torch.tensor([[0.3, 0.2, 0.1]], dtype=torch.float32, device=device)\n",
        "    y_valid = ground_truth(x_valid)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        y_hat = model(x_valid)\n",
        "    model.train()\n",
        "\n",
        "    print(avg_loss, y_valid[0].item(), y_hat[0, 0].item())\n",
        "\n",
        "    if avg_loss < 1e-3:\n",
        "        break"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}